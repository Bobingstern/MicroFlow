{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v7853SvnCJwN"
      },
      "outputs": [],
      "source": [
        "# TensorFlow is an open source machine learning library\n",
        "import tensorflow as tf\n",
        "# Keras is TensorFlow's high-level API for deep learning\n",
        "from tensorflow import keras\n",
        "# Numpy is a math library\n",
        "import numpy as np\n",
        "# Pandas is a data manipulation library \n",
        "import pandas as pd\n",
        "# Matplotlib is a graphing library\n",
        "import matplotlib.pyplot as plt\n",
        "# Math is Python's math library\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load training data\n",
        "#This is a horrifically bad way of doing it but google colab hates file reading and stuff so whatever\n",
        "training_inputs = [[15], [15], [15], [14], [13], [12], [11], [10], [9], [8], [7], [6], [5], [4], [3], [2], [1], [0], [5], [4], [3], [2], [1], [0], [6], [5], [4], [3], [2], [1], [0], [5], [4], [3], [2], [1], [0], [3], [2], [1], [0], [5], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [8], [7], [6], [5], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [8], [7], [6], [5], [4], [3], [2], [1], [0], [3], [2], [1], [0], [4], [3], [2], [1], [0], [2], [1], [0], [4], [3], [2], [1], [0], [5], [4], [3], [2], [1], [0], [9], [8], [7], [6], [5], [4], [3], [2], [1], [0], [7], [6], [5], [4], [3], [2], [1], [0], [2], [1], [0], [12], [11], [10], [9], [8], [7], [6], [5], [4], [3], [2], [1], [0], [8], [7], [6], [5], [4], [3], [2], [1], [0], [2], [1], [0], [2], [1], [0], [6], [5], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [2], [1], [0], [5], [4], [3], [2], [1], [0], [3], [2], [1], [0], [2], [1], [0], [8], [7], [6], [5], [4], [3], [2], [1], [0], [2], [1], [0], [2], [1], [0], [2], [1], [0], [4], [3], [2], [1], [0], [13], [12], [11], [10], [9], [8], [7], [6], [5], [4], [3], [2], [1], [0], [2], [1], [0], [4], [3], [2], [1], [0], [2], [1], [0], [3], [2], [1], [0], [2], [1], [0], [6], [5], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [2], [1], [0], [3], [2], [1], [0], [2], [1], [0], [9], [8], [7], [6], [5], [4], [3], [2], [1], [0], [4], [3], [2], [1], [0], [3], [2], [1], [0], [2], [1], [0], [5], [4], [3], [2], [1], [0], [5], [4], [3], [2], [1], [0], [2], [1], [0], [2], [1], [0], [2], [1], [0], [5], [4], [3], [2], [1]]\n",
        "training_outputs = [[1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0]]\n",
        "\n",
        "training_inputs = np.array(training_inputs)\n",
        "training_outputs = np.array(training_outputs)"
      ],
      "metadata": {
        "id": "vA-_a_ecCSmz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use Keras to create a simple model architecture\n",
        "model = tf.keras.Sequential()\n",
        "# First layer takes a scalar input and feeds it through 8 \"neurons\". The\n",
        "# neurons decide whether to activate based on the 'relu' activation function.\n",
        "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(1,)))\n",
        "model.add(keras.layers.Dense(16, activation='sigmoid'))\n",
        "# Final layer is a single neuron, since we want to output a single value\n",
        "model.add(keras.layers.Dense(1))\n",
        "\n",
        "# Compile the model using the standard 'adam' optimizer and the mean squared error or 'mse' loss function for regression.\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "kxL8yC2HESQ4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model.fit(training_inputs, training_outputs, epochs=1000, batch_size=64)"
      ],
      "metadata": {
        "id": "RmabVR5UEaKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_to_cpp(model, filename=\"weights_and_biases.txt\"):\n",
        "    model.summary()\n",
        "    weights = []\n",
        "    biases = []\n",
        "    for l in range(len(model.layers)):\n",
        "        W, B = model.layers[l].get_weights()\n",
        "        weights.append(W.flatten())\n",
        "        biases.append(B.flatten())\n",
        "    \n",
        "    z = []\n",
        "    b = []\n",
        "    for i in np.array(weights):\n",
        "        for l in i:\n",
        "            z.append(l)\n",
        "    for i in np.array(biases):\n",
        "        for l in i:\n",
        "            b.append(l)\n",
        "    with open(filename, \"w\") as f:\n",
        "      f.write(\"weights: {\")\n",
        "      for i in range(len(z)):\n",
        "        if (i < len(z)-1):\n",
        "          f.write(str(z[i])+\", \")\n",
        "        else:\n",
        "          f.write(str(z[i]))\n",
        "      f.write(\"}\\n\\n\")\n",
        "\n",
        "      f.write(\"biases: {\")\n",
        "      for i in range(len(b)):\n",
        "        if (i < len(b)-1):\n",
        "          f.write(str(b[i])+\", \")\n",
        "        else:\n",
        "          f.write(str(b[i]))\n",
        "      f.write(\"}\\n\\n\")\n",
        "    \n",
        "      arch = []\n",
        "    \n",
        "      arch.append(model.layers[0].input_shape[1])\n",
        "      for i in range(1, len(model.layers)):\n",
        "          arch.append(model.layers[i].input_shape[1])\n",
        "      arch.append(model.layers[len(model.layers)-1].output_shape[1])\n",
        "      f.write(\"Architecture: {\")\n",
        "      for i in range(len(arch)):\n",
        "          if (i < len(arch)-1):\n",
        "              f.write(str(arch[i])+\", \")\n",
        "          else:\n",
        "              f.write(str(arch[i]))\n",
        "      f.write(\"}\")\n",
        "      print(\"Architecture (alpha):\", arch)\n",
        "      print(\"Layers: \", len(arch))\n",
        "    print(\"Weights: \", z)\n",
        "    print(\"Biases: \", b)\n",
        "    \n",
        "weights_to_cpp(model)"
      ],
      "metadata": {
        "id": "PMk9-1hSFADT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
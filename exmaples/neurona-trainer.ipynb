{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TensorFlow is an open source machine learning library\nimport tensorflow as tf\n\n# Keras is TensorFlow's high-level API for deep learning\nfrom tensorflow import keras\n# Numpy is a math library\nimport numpy as np\n# Pandas is a data manipulation library \nimport pandas as pd\n# Matplotlib is a graphing library\nimport matplotlib.pyplot as plt\n# Math is Python's math library\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-08T13:22:49.699942Z","iopub.execute_input":"2022-03-08T13:22:49.700391Z","iopub.status.idle":"2022-03-08T13:22:51.987033Z","shell.execute_reply.started":"2022-03-08T13:22:49.700286Z","shell.execute_reply":"2022-03-08T13:22:51.985532Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Number of sample datapoints\nSAMPLES = 1000\n\n# Generate a uniformly distributed set of random numbers in the range from\n# 0 to 2Ï€, which covers a complete sine wave oscillation\nx_values = np.random.uniform(\n    low=0, high=2*math.pi, size=SAMPLES).astype(np.float32)\n\n# Shuffle the values to guarantee they're not in order\nnp.random.shuffle(x_values)\n\n# Calculate the corresponding sine values\ny_values = np.sin(x_values).astype(np.float32)\n\n# Plot our data. The 'b.' argument tells the library to print blue dots.\nplt.plot(x_values, y_values, 'b.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:36:36.379779Z","iopub.execute_input":"2022-03-07T22:36:36.380074Z","iopub.status.idle":"2022-03-07T22:36:36.617903Z","shell.execute_reply.started":"2022-03-07T22:36:36.380044Z","shell.execute_reply":"2022-03-07T22:36:36.616885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a small random number to each y value\ny_values += 0.1 * np.random.randn(*y_values.shape)\n\n# Plot our data\nplt.plot(x_values, y_values, 'b.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:36:44.415545Z","iopub.execute_input":"2022-03-07T22:36:44.41587Z","iopub.status.idle":"2022-03-07T22:36:44.596104Z","shell.execute_reply.started":"2022-03-07T22:36:44.415816Z","shell.execute_reply":"2022-03-07T22:36:44.595093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll use 60% of our data for training and 20% for testing. The remaining 20%\n# will be used for validation. Calculate the indices of each section.\nTRAIN_SPLIT =  int(0.6 * SAMPLES)\nTEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n\n# Use np.split to chop our data into three parts.\n# The second argument to np.split is an array of indices where the data will be\n# split. We provide two indices, so the data will be divided into three chunks.\nx_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\ny_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n# Double check that our splits add up correctly\nassert (x_train.size + x_validate.size + x_test.size) ==  SAMPLES\n\n# Plot the data in each partition in different colors:\nplt.plot(x_train, y_train, 'b.', label=\"Train\")\nplt.plot(x_test, y_test, 'r.', label=\"Test\")\nplt.plot(x_validate, y_validate, 'y.', label=\"Validate\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:36:46.746499Z","iopub.execute_input":"2022-03-07T22:36:46.74681Z","iopub.status.idle":"2022-03-07T22:36:46.967992Z","shell.execute_reply.started":"2022-03-07T22:36:46.746766Z","shell.execute_reply":"2022-03-07T22:36:46.967373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll use Keras to create a simple model architecture\nmodel = tf.keras.Sequential()\n\n# First layer takes a scalar input and feeds it through 8 \"neurons\". The\n# neurons decide whether to activate based on the 'relu' activation function.\nmodel.add(keras.layers.Dense(16, activation='tanh', input_shape=(1,)))\nmodel.add(keras.layers.Dense(16, activation='tanh'))\n# Final layer is a single neuron, since we want to output a single value\nmodel.add(keras.layers.Dense(1))\n\n# Compile the model using the standard 'adam' optimizer and the mean squared error or 'mse' loss function for regression.\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:36:49.596329Z","iopub.execute_input":"2022-03-07T22:36:49.597167Z","iopub.status.idle":"2022-03-07T22:36:50.889101Z","shell.execute_reply.started":"2022-03-07T22:36:49.59712Z","shell.execute_reply":"2022-03-07T22:36:50.888424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model on our training data while validating on our validation set\nhistory_1 = model.fit(x_train, y_train, epochs=500, batch_size=64,\n                        validation_data=(x_validate, y_validate))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:36:53.022682Z","iopub.execute_input":"2022-03-07T22:36:53.023594Z","iopub.status.idle":"2022-03-07T22:37:23.396826Z","shell.execute_reply.started":"2022-03-07T22:36:53.023545Z","shell.execute_reply":"2022-03-07T22:37:23.396064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculate and print the loss on our test dataset\ntest_loss, test_mae = model.evaluate(x_test, y_test)\n\n# Make predictions based on our test dataset\ny_test_pred = model.predict(x_test)\n\n# Graph the predictions against the actual values\nplt.clf()\nplt.title('Comparison of predictions and actual values')\nplt.plot(x_test, y_test, 'b.', label='Actual values')\nplt.plot(x_test, y_test_pred, 'r.', label='TF predictions')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:37:43.024368Z","iopub.execute_input":"2022-03-07T22:37:43.025047Z","iopub.status.idle":"2022-03-07T22:37:43.459688Z","shell.execute_reply.started":"2022-03-07T22:37:43.025Z","shell.execute_reply":"2022-03-07T22:37:43.458834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"sin_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nmodel.summary()\nN = model.get_weights()\nweights = []\nfor i in N:\n    for j in i: \n        if type(j) == np.ndarray:\n            for z in j:\n                weights.append(z)\n        else:\n            weights.append(j)\nprint(N)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = keras.Sequential()\nMODEL.add(keras.layers.Dense(2, activation='sigmoid', input_shape=(2,)))\nMODEL.add(keras.layers.Dense(2, activation='sigmoid'))\n# Final layer is a single neuron, since we want to output a single value\nMODEL.add(keras.layers.Dense(1))\n\n# Compile the model using the standard 'adam' optimizer and the mean squared error or 'mse' loss function for regression.\nMODEL.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-2), loss='mse')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-08T13:22:58.615972Z","iopub.execute_input":"2022-03-08T13:22:58.616591Z","iopub.status.idle":"2022-03-08T13:22:58.878253Z","shell.execute_reply.started":"2022-03-08T13:22:58.616553Z","shell.execute_reply":"2022-03-08T13:22:58.877062Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"training_data = np.array([[0, 0],[1, 0],[0, 1],[1, 1]], \"float32\")\n\n# the four expected results in the same order\ntarget_data = np.array([[0],[1],[1],[0]], \"float32\")\n\nMODEL.fit(training_data, target_data, epochs=1000, verbose=2)\n\nprint(MODEL.predict(training_data).round())","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-08T13:23:02.449283Z","iopub.execute_input":"2022-03-08T13:23:02.449573Z","iopub.status.idle":"2022-03-08T13:23:08.103840Z","shell.execute_reply.started":"2022-03-08T13:23:02.449545Z","shell.execute_reply":"2022-03-08T13:23:08.102863Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nmodel.summary()\n#MODEL.save(\"XOR\")\n\ndef export_weights(model):\n    weights = []\n    biases = []\n    for l in range(len(model.layers)):\n        W, B = model.layers[l].get_weights()\n        weights.append(W)\n        biases.append(B)\n    return weights, biases\n\ndef export_weights_flattened(model):\n    weights = []\n    biases = []\n    for l in range(len(model.layers)):\n        W, B = model.layers[l].get_weights()\n        weights.append(W.flatten())\n        biases.append(B.flatten())\n    \n    z = []\n    b = []\n    for i in np.array(weights):\n        for l in i:\n            z.append(l)\n    for i in np.array(biases):\n        for l in i:\n            b.append(l)\n    return z, b\n\ndef activate(x):\n    return 1/(1 + np.exp(-x))\n\ndef calculate_output(w_b, b, input_):\n    weights = w_b\n    x = np.array(input_)\n    print(x.shape)\n    x = x.dot(weights)\n    print(x)\n    return x\n\nprint(export_weights_flattened(model)[0])\nprint(export_weights_flattened(model)[1])\n# z = export_weights(MODEL)\n# x = np.array([1])\n# #print(z[0][0])\n# x = x @ z[0][0] + z[1][0]\n# x = np.array([np.tanh(i) for i in x])\n\n# # print(x)\n# # x = x @ z[0][1] + z[1][1]\n# #x = np.array([np.tanh(i) for i in x])\n\n# # x = x @ z[0][2] + z[1][2]\n# # #x = np.array([np.tanh(i) for i in x])\n# print(x)\n# print(MODEL.predict([[1]]))\n#calculate_output(z[0][0], z[1][0], [1, 0])","metadata":{"execution":{"iopub.status.busy":"2022-03-08T13:23:16.933049Z","iopub.execute_input":"2022-03-08T13:23:16.933930Z","iopub.status.idle":"2022-03-08T13:23:17.263831Z","shell.execute_reply.started":"2022-03-08T13:23:16.933888Z","shell.execute_reply":"2022-03-08T13:23:17.262791Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def weights_to_cpp(model, filename=\"weights_and_biases.txt\"):\n    model.summary()\n    weights = []\n    biases = []\n    for l in range(len(model.layers)):\n        W, B = model.layers[l].get_weights()\n        weights.append(W.flatten())\n        biases.append(B.flatten())\n    \n    z = []\n    b = []\n    for i in np.array(weights):\n        for l in i:\n            z.append(l)\n    for i in np.array(biases):\n        for l in i:\n            b.append(l)\n    with open(filename, \"w\") as f:\n      f.write(\"weights: {\")\n      for i in range(len(z)):\n        if (i < len(z)-1):\n          f.write(str(z[i])+\", \")\n        else:\n          f.write(str(z[i]))\n      f.write(\"}\\n\\n\")\n\n      f.write(\"biases: {\")\n      for i in range(len(b)):\n        if (i < len(b)-1):\n          f.write(str(b[i])+\", \")\n        else:\n          f.write(str(b[i]))\n      f.write(\"}\\n\\n\")\n    \n      arch = []\n    \n      arch.append(model.layers[0].input_shape[1])\n      for i in range(1, len(model.layers)):\n          arch.append(model.layers[i].input_shape[1])\n      arch.append(model.layers[len(model.layers)-1].output_shape[1])\n      f.write(\"Architecture: {\")\n      for i in range(len(arch)):\n          if (i < len(arch)-1):\n              f.write(str(arch[i])+\", \")\n          else:\n              f.write(str(arch[i]))\n      f.write(\"}\")\n      print(\"Architecture (alpha):\", arch)\n    print(\"Weights: \", z)\n    print(\"Biases: \", b)\n    \nweights_to_cpp(MODEL)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T13:26:33.243825Z","iopub.execute_input":"2022-03-08T13:26:33.244949Z","iopub.status.idle":"2022-03-08T13:26:33.269612Z","shell.execute_reply.started":"2022-03-08T13:26:33.244890Z","shell.execute_reply":"2022-03-08T13:26:33.268767Z"},"trusted":true},"execution_count":8,"outputs":[]}]}